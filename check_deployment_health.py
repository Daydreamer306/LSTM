#!/usr/bin/env python3
"""
PAI-DSWéƒ¨ç½²çŠ¶æ€æ£€æŸ¥è„šæœ¬
æ£€æŸ¥é¡¹ç›®åœ¨PAI-DSWç¯å¢ƒä¸­çš„éƒ¨ç½²çŠ¶æ€å’Œè¿è¡Œå¥åº·åº¦
"""

import os
import sys
import json
import subprocess
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any

def check_pai_dsw_environment() -> Dict[str, Any]:
    """æ£€æŸ¥PAI-DSWç¯å¢ƒ"""
    env_info = {
        'is_pai_dsw': False,
        'workspace_path': None,
        'python_version': sys.version,
        'working_directory': os.getcwd()
    }
    
    if os.path.exists('/mnt/workspace'):
        env_info['is_pai_dsw'] = True
        env_info['workspace_path'] = '/mnt/workspace'
        
    return env_info

def check_gpu_status() -> Dict[str, Any]:
    """æ£€æŸ¥GPUçŠ¶æ€"""
    gpu_info = {
        'cuda_available': False,
        'gpu_count': 0,
        'gpu_models': [],
        'gpu_memory': []
    }
    
    try:
        import torch
        gpu_info['cuda_available'] = torch.cuda.is_available()
        
        if torch.cuda.is_available():
            gpu_info['gpu_count'] = torch.cuda.device_count()
            
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
                gpu_info['gpu_models'].append(gpu_name)
                gpu_info['gpu_memory'].append(f"{gpu_memory:.1f}GB")
                
    except ImportError:
        pass
    
    return gpu_info

def check_dependencies() -> Dict[str, bool]:
    """æ£€æŸ¥å…³é”®ä¾èµ–åŒ…"""
    critical_packages = [
        'torch', 'numpy', 'pandas', 'matplotlib', 
        'seaborn', 'sklearn', 'yaml', 'tqdm'
    ]
    
    dep_status = {}
    for package in critical_packages:
        try:
            __import__(package)
            dep_status[package] = True
        except ImportError:
            dep_status[package] = False
            
    return dep_status

def check_project_structure() -> Dict[str, bool]:
    """æ£€æŸ¥é¡¹ç›®æ–‡ä»¶ç»“æ„"""
    required_files = [
        'src/__init__.py',
        'src/data/__init__.py',
        'src/models/__init__.py',
        'src/training/__init__.py',
        'src/inference/__init__.py',
        'src/utils/__init__.py',
        'configs/lstm_transformer.yaml',
        'requirements.txt',
        'setup_pai_dsw.py'
    ]
    
    structure_status = {}
    for file_path in required_files:
        structure_status[file_path] = os.path.exists(file_path)
        
    return structure_status

def check_data_files() -> Dict[str, Any]:
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶çŠ¶æ€"""
    data_info = {
        'raw_data_found': False,
        'processed_data_found': False,
        'raw_data_files': [],
        'processed_data_files': []
    }
    
    # æ£€æŸ¥åŸå§‹æ•°æ®
    raw_data_paths = ['data/raw/*.csv', 'data/*.csv']
    for pattern in raw_data_paths:
        import glob
        files = glob.glob(pattern)
        if files:
            data_info['raw_data_found'] = True
            data_info['raw_data_files'].extend(files)
            
    # æ£€æŸ¥é¢„å¤„ç†æ•°æ®
    processed_paths = ['data/processed/*.npz']
    for pattern in processed_paths:
        files = glob.glob(pattern)
        if files:
            data_info['processed_data_found'] = True
            data_info['processed_data_files'].extend(files)
            
    return data_info

def check_model_files() -> Dict[str, Any]:
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶çŠ¶æ€"""
    model_info = {
        'trained_model_exists': False,
        'onnx_model_exists': False,
        'checkpoint_files': [],
        'model_size_mb': 0
    }
    
    # æ£€æŸ¥è®­ç»ƒå¥½çš„æ¨¡å‹
    model_path = 'models/best_model.pt'
    if os.path.exists(model_path):
        model_info['trained_model_exists'] = True
        model_info['model_size_mb'] = os.path.getsize(model_path) / (1024 * 1024)
        
    # æ£€æŸ¥ONNXæ¨¡å‹
    onnx_path = 'models/model.onnx'
    if os.path.exists(onnx_path):
        model_info['onnx_model_exists'] = True
        
    # æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶
    if os.path.exists('checkpoints'):
        import glob
        checkpoint_files = glob.glob('checkpoints/*.pt')
        model_info['checkpoint_files'] = checkpoint_files
        
    return model_info

def check_training_status() -> Dict[str, Any]:
    """æ£€æŸ¥è®­ç»ƒçŠ¶æ€"""
    training_info = {
        'training_history_exists': False,
        'training_completed': False,
        'last_epoch': 0,
        'best_metrics': {}
    }
    
    history_path = 'results/training_history.json'
    if os.path.exists(history_path):
        training_info['training_history_exists'] = True
        
        try:
            with open(history_path, 'r') as f:
                history = json.load(f)
                
            if 'train_loss' in history and len(history['train_loss']) > 0:
                training_info['training_completed'] = True
                training_info['last_epoch'] = len(history['train_loss'])
                
                # æå–æœ€ä½³æŒ‡æ ‡
                if 'val_mae' in history:
                    training_info['best_metrics']['val_mae'] = min(history['val_mae'])
                if 'val_r2' in history:
                    training_info['best_metrics']['val_r2'] = max(history['val_r2'])
                    
        except Exception as e:
            print(f"è§£æè®­ç»ƒå†å²å¤±è´¥: {e}")
            
    return training_info

def check_results_and_outputs() -> Dict[str, Any]:
    """æ£€æŸ¥ç»“æœå’Œè¾“å‡ºæ–‡ä»¶"""
    results_info = {
        'evaluation_metrics_exists': False,
        'visualizations_found': False,
        'analysis_reports_found': False,
        'visualization_count': 0
    }
    
    # æ£€æŸ¥è¯„ä¼°æŒ‡æ ‡
    metrics_path = 'results/evaluation_metrics.json'
    if os.path.exists(metrics_path):
        results_info['evaluation_metrics_exists'] = True
        
    # æ£€æŸ¥å¯è§†åŒ–æ–‡ä»¶
    if os.path.exists('results'):
        import glob
        viz_files = glob.glob('results/*.png')
        if viz_files:
            results_info['visualizations_found'] = True
            results_info['visualization_count'] = len(viz_files)
            
    # æ£€æŸ¥åˆ†ææŠ¥å‘Š
    if os.path.exists('analysis'):
        report_files = glob.glob('analysis/*.json') + glob.glob('analysis/*.md')
        if report_files:
            results_info['analysis_reports_found'] = True
            
    return results_info

def check_system_resources() -> Dict[str, Any]:
    """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
    resource_info = {
        'disk_usage_gb': 0,
        'available_space_gb': 0,
        'memory_usage_gb': 0
    }
    
    try:
        import shutil
        total, used, free = shutil.disk_usage('/')
        resource_info['disk_usage_gb'] = used / 1e9
        resource_info['available_space_gb'] = free / 1e9
    except:
        pass
        
    try:
        import psutil
        memory = psutil.virtual_memory()
        resource_info['memory_usage_gb'] = memory.used / 1e9
    except ImportError:
        pass
        
    return resource_info

def run_quick_functionality_test() -> Dict[str, bool]:
    """è¿è¡Œå¿«é€ŸåŠŸèƒ½æµ‹è¯•"""
    test_results = {}
    
    # æµ‹è¯•1: æ•°æ®åŠ è½½
    try:
        from src.data.quick_check import quick_data_check
        result = quick_data_check()
        test_results['data_loading'] = result
    except Exception as e:
        test_results['data_loading'] = False
        
    # æµ‹è¯•2: æ¨¡å‹åˆ›å»º
    try:
        from src.models.model_utils import ModelFactory
        factory = ModelFactory()
        config = {'input_size': 1, 'hidden_size': 64, 'sequence_length': 96}
        model = factory.create_model(config, 'basic')
        test_results['model_creation'] = True
    except Exception as e:
        test_results['model_creation'] = False
        
    # æµ‹è¯•3: é…ç½®åŠ è½½
    try:
        import yaml
        with open('configs/lstm_transformer.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        test_results['config_loading'] = True
    except Exception as e:
        test_results['config_loading'] = False
        
    return test_results

def generate_health_report() -> Dict[str, Any]:
    """ç”Ÿæˆå®Œæ•´çš„å¥åº·æ£€æŸ¥æŠ¥å‘Š"""
    print("ğŸ” å¼€å§‹PAI-DSWéƒ¨ç½²çŠ¶æ€æ£€æŸ¥...")
    
    report = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'environment': check_pai_dsw_environment(),
        'gpu': check_gpu_status(),
        'dependencies': check_dependencies(),
        'project_structure': check_project_structure(),
        'data_files': check_data_files(),
        'model_files': check_model_files(),
        'training_status': check_training_status(),
        'results': check_results_and_outputs(),
        'system_resources': check_system_resources(),
        'functionality_tests': run_quick_functionality_test()
    }
    
    return report

def print_health_summary(report: Dict[str, Any]):
    """æ‰“å°å¥åº·æ£€æŸ¥æ‘˜è¦"""
    print("\n" + "="*60)
    print("PAI-DSW éƒ¨ç½²çŠ¶æ€æ£€æŸ¥æŠ¥å‘Š")
    print("="*60)
    
    # ç¯å¢ƒä¿¡æ¯
    env = report['environment']
    print(f"\nğŸ“‹ ç¯å¢ƒä¿¡æ¯:")
    print(f"  PAI-DSWç¯å¢ƒ: {'âœ… æ˜¯' if env['is_pai_dsw'] else 'âŒ å¦'}")
    print(f"  å·¥ä½œç›®å½•: {env['working_directory']}")
    print(f"  Pythonç‰ˆæœ¬: {env['python_version'].split()[0]}")
    
    # GPUä¿¡æ¯
    gpu = report['gpu']
    print(f"\nğŸ–¥ï¸  GPUä¿¡æ¯:")
    if gpu['cuda_available']:
        print(f"  CUDAå¯ç”¨: âœ… æ˜¯ ({gpu['gpu_count']} ä¸ªGPU)")
        for i, (model, memory) in enumerate(zip(gpu['gpu_models'], gpu['gpu_memory'])):
            print(f"    GPU {i}: {model} ({memory})")
    else:
        print(f"  CUDAå¯ç”¨: âŒ å¦ (å°†ä½¿ç”¨CPU)")
    
    # ä¾èµ–çŠ¶æ€
    deps = report['dependencies']
    print(f"\nğŸ“¦ å…³é”®ä¾èµ–:")
    missing_deps = [pkg for pkg, status in deps.items() if not status]
    if missing_deps:
        print(f"  âŒ ç¼ºå¤±ä¾èµ–: {', '.join(missing_deps)}")
    else:
        print(f"  âœ… æ‰€æœ‰å…³é”®ä¾èµ–å·²å®‰è£…")
    
    # é¡¹ç›®ç»“æ„
    struct = report['project_structure']
    print(f"\nğŸ“ é¡¹ç›®ç»“æ„:")
    missing_files = [file for file, status in struct.items() if not status]
    if missing_files:
        print(f"  âŒ ç¼ºå¤±æ–‡ä»¶: {len(missing_files)} ä¸ª")
        for file in missing_files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"    - {file}")
    else:
        print(f"  âœ… é¡¹ç›®ç»“æ„å®Œæ•´")
    
    # æ•°æ®çŠ¶æ€
    data = report['data_files']
    print(f"\nğŸ’¾ æ•°æ®çŠ¶æ€:")
    print(f"  åŸå§‹æ•°æ®: {'âœ… å·²å‘ç°' if data['raw_data_found'] else 'âŒ æœªå‘ç°'}")
    print(f"  é¢„å¤„ç†æ•°æ®: {'âœ… å·²ç”Ÿæˆ' if data['processed_data_found'] else 'âŒ æœªç”Ÿæˆ'}")
    
    # æ¨¡å‹çŠ¶æ€
    model = report['model_files']
    print(f"\nğŸ¤– æ¨¡å‹çŠ¶æ€:")
    print(f"  è®­ç»ƒæ¨¡å‹: {'âœ… å­˜åœ¨' if model['trained_model_exists'] else 'âŒ ä¸å­˜åœ¨'}")
    if model['trained_model_exists']:
        print(f"    å¤§å°: {model['model_size_mb']:.1f} MB")
    print(f"  ONNXæ¨¡å‹: {'âœ… å­˜åœ¨' if model['onnx_model_exists'] else 'âŒ ä¸å­˜åœ¨'}")
    
    # è®­ç»ƒçŠ¶æ€
    training = report['training_status']
    print(f"\nğŸš€ è®­ç»ƒçŠ¶æ€:")
    print(f"  è®­ç»ƒå†å²: {'âœ… å­˜åœ¨' if training['training_history_exists'] else 'âŒ ä¸å­˜åœ¨'}")
    print(f"  è®­ç»ƒå®Œæˆ: {'âœ… æ˜¯' if training['training_completed'] else 'âŒ å¦'}")
    if training['training_completed']:
        print(f"    è®­ç»ƒè½®æ•°: {training['last_epoch']}")
        for metric, value in training['best_metrics'].items():
            print(f"    æœ€ä½³{metric}: {value:.6f}")
    
    # ç»“æœçŠ¶æ€
    results = report['results']
    print(f"\nğŸ“Š ç»“æœçŠ¶æ€:")
    print(f"  è¯„ä¼°æŒ‡æ ‡: {'âœ… å­˜åœ¨' if results['evaluation_metrics_exists'] else 'âŒ ä¸å­˜åœ¨'}")
    print(f"  å¯è§†åŒ–å›¾è¡¨: {'âœ… å­˜åœ¨' if results['visualizations_found'] else 'âŒ ä¸å­˜åœ¨'}")
    if results['visualizations_found']:
        print(f"    å›¾è¡¨æ•°é‡: {results['visualization_count']} ä¸ª")
    
    # åŠŸèƒ½æµ‹è¯•
    tests = report['functionality_tests']
    print(f"\nğŸ§ª åŠŸèƒ½æµ‹è¯•:")
    passed_tests = sum(1 for result in tests.values() if result)
    total_tests = len(tests)
    print(f"  æµ‹è¯•ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡")
    
    for test_name, result in tests.items():
        status = "âœ…" if result else "âŒ"
        print(f"    {status} {test_name}")
    
    # ç³»ç»Ÿèµ„æº
    resources = report['system_resources']
    print(f"\nğŸ’» ç³»ç»Ÿèµ„æº:")
    if resources['available_space_gb'] > 0:
        print(f"  å¯ç”¨å­˜å‚¨: {resources['available_space_gb']:.1f} GB")
        if resources['available_space_gb'] < 5:
            print(f"    âš ï¸  å­˜å‚¨ç©ºé—´ä¸è¶³")
    
    # ç»¼åˆè¯„åˆ†
    total_checks = 0
    passed_checks = 0
    
    # ç¯å¢ƒæ£€æŸ¥
    total_checks += 1
    if env['is_pai_dsw']:
        passed_checks += 1
        
    # ä¾èµ–æ£€æŸ¥
    total_checks += len(deps)
    passed_checks += sum(1 for status in deps.values() if status)
    
    # åŠŸèƒ½æµ‹è¯•
    total_checks += len(tests)
    passed_checks += sum(1 for result in tests.values() if result)
    
    # å…³é”®æ–‡ä»¶æ£€æŸ¥
    total_checks += 4  # æ•°æ®ã€æ¨¡å‹ã€è®­ç»ƒã€ç»“æœ
    if data['raw_data_found']:
        passed_checks += 1
    if model['trained_model_exists']:
        passed_checks += 1
    if training['training_completed']:
        passed_checks += 1
    if results['evaluation_metrics_exists']:
        passed_checks += 1
        
    health_score = (passed_checks / total_checks) * 100
    
    print(f"\nğŸ¯ éƒ¨ç½²å¥åº·åº¦è¯„åˆ†: {health_score:.1f}% ({passed_checks}/{total_checks})")
    
    if health_score >= 90:
        print("ğŸ‰ éƒ¨ç½²çŠ¶æ€ä¼˜ç§€! æ‰€æœ‰åŠŸèƒ½æ­£å¸¸è¿è¡Œ")
    elif health_score >= 70:
        print("ğŸ‘ éƒ¨ç½²çŠ¶æ€è‰¯å¥½ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨")
    elif health_score >= 50:
        print("âš ï¸  éƒ¨ç½²çŠ¶æ€ä¸€èˆ¬ï¼Œå»ºè®®æ£€æŸ¥é—®é¢˜é¡¹")
    else:
        print("âŒ éƒ¨ç½²çŠ¶æ€è¾ƒå·®ï¼Œéœ€è¦è§£å†³å…³é”®é—®é¢˜")

def save_report(report: Dict[str, Any]):
    """ä¿å­˜æ£€æŸ¥æŠ¥å‘Š"""
    report_path = 'deployment_health_report.json'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
    print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # ç”Ÿæˆå¥åº·æ£€æŸ¥æŠ¥å‘Š
        report = generate_health_report()
        
        # æ‰“å°æ‘˜è¦
        print_health_summary(report)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        save_report(report)
        
        # æä¾›æ”¹è¿›å»ºè®®
        print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        
        if not report['data_files']['raw_data_found']:
            print("  1. ä¸Šä¼ è®­ç»ƒæ•°æ®åˆ° data/raw/ ç›®å½•")
            
        if not report['model_files']['trained_model_exists']:
            print("  2. è¿è¡Œæ¨¡å‹è®­ç»ƒ: python src/training/train.py --config configs/lstm_transformer.yaml")
            
        if not report['results']['evaluation_metrics_exists']:
            print("  3. è¿è¡Œæ¨¡å‹è¯„ä¼°: python src/inference/predict.py")
            
        if not all(report['functionality_tests'].values()):
            print("  4. æ£€æŸ¥åŠŸèƒ½æµ‹è¯•å¤±è´¥çš„æ¨¡å—")
            
        print("\nğŸ“š æ›´å¤šå¸®åŠ©:")
        print("  - æŸ¥çœ‹éƒ¨ç½²æŒ‡å—: PAI_DSW_DEPLOYMENT.md")
        print("  - è¿è¡Œå¿«é€Ÿå¯åŠ¨: PAI_DSW_QuickStart.ipynb")
        print("  - ä¸€é”®éƒ¨ç½²è„šæœ¬: bash deploy_pai_dsw_full.sh")
        
    except Exception as e:
        print(f"âŒ å¥åº·æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
if __name__ == "__main__":
    main()
