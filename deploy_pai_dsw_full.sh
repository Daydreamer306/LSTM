#!/bin/bash
# PAI-DSW ä¸€é”®éƒ¨ç½²è„šæœ¬
# åœ¨PAI-DSWçŽ¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬è¿›è¡Œå®Œæ•´çš„é¡¹ç›®åˆå§‹åŒ–å’Œéƒ¨ç½²

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "ðŸš€ å¼€å§‹ LSTM+Transformer å·¥ä¸šæ—¶åºé¢„æµ‹æ¨¡åž‹ PAI-DSW éƒ¨ç½²"
echo "============================================================"

# æ£€æŸ¥PAI-DSWçŽ¯å¢ƒ
echo "ðŸ“‹ 1. æ£€æŸ¥PAI-DSWçŽ¯å¢ƒ..."
if [ ! -d "/mnt/workspace" ]; then
    echo "âŒ é”™è¯¯: æœªæ£€æµ‹åˆ°PAI-DSWå·¥ä½œç©ºé—´çŽ¯å¢ƒ (/mnt/workspace ä¸å­˜åœ¨)"
    echo "è¯·ç¡®ä¿åœ¨PAI-DSWå®žä¾‹ä¸­è¿è¡Œæ­¤è„šæœ¬"
    exit 1
fi

# è¿›å…¥å·¥ä½œç›®å½•
cd /mnt/workspace
export PYTHONPATH=/mnt/workspace:$PYTHONPATH

echo "âœ… PAI-DSWçŽ¯å¢ƒæ£€æŸ¥é€šè¿‡"
echo "å·¥ä½œç›®å½•: $(pwd)"

# æ£€æŸ¥PythonçŽ¯å¢ƒ
echo ""
echo "ðŸ 2. æ£€æŸ¥PythonçŽ¯å¢ƒ..."
python_version=$(python --version 2>&1)
echo "Pythonç‰ˆæœ¬: $python_version"

# æ£€æŸ¥GPU
echo ""
echo "ðŸ–¥ï¸  3. æ£€æŸ¥GPUçŽ¯å¢ƒ..."
if command -v nvidia-smi &> /dev/null; then
    echo "GPUä¿¡æ¯:"
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits
else
    echo "âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ"
fi

# åˆ›å»ºç›®å½•ç»“æž„
echo ""
echo "ðŸ“ 4. åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æž„..."
directories=(
    "data/raw"
    "data/processed" 
    "data/external"
    "models"
    "results/training"
    "results/predictions"
    "results/visualizations"
    "logs"
    "checkpoints"
    "analysis"
    "exports"
)

for dir in "${directories[@]}"; do
    mkdir -p "$dir"
    echo "âœ… åˆ›å»ºç›®å½•: $dir"
done

# æ£€æŸ¥é¡¹ç›®æ–‡ä»¶
echo ""
echo "ðŸ—‚ï¸  5. æ£€æŸ¥é¡¹ç›®æ–‡ä»¶..."
required_files=(
    "setup_pai_dsw.py"
    "requirements.txt"
    "configs/lstm_transformer.yaml"
    "src/__init__.py"
    "src/data/__init__.py"
    "src/models/__init__.py"
    "src/training/__init__.py"
    "src/inference/__init__.py"
    "src/utils/__init__.py"
)

missing_files=()
for file in "${required_files[@]}"; do
    if [ -f "$file" ]; then
        echo "âœ… $file"
    else
        echo "âŒ $file - ç¼ºå¤±"
        missing_files+=("$file")
    fi
done

if [ ${#missing_files[@]} -gt 0 ]; then
    echo ""
    echo "âŒ é”™è¯¯: ä»¥ä¸‹å…³é”®æ–‡ä»¶ç¼ºå¤±:"
    printf '%s\n' "${missing_files[@]}"
    echo ""
    echo "è¯·ç¡®ä¿å·²å°†å®Œæ•´çš„é¡¹ç›®æ–‡ä»¶ä¸Šä¼ åˆ° /mnt/workspace ç›®å½•"
    exit 1
fi

echo "âœ… é¡¹ç›®æ–‡ä»¶æ£€æŸ¥é€šè¿‡"

# å®‰è£…ä¾èµ–
echo ""
echo "ðŸ“¦ 6. å®‰è£…Pythonä¾èµ–åŒ…..."

# ä½¿ç”¨å›½å†…é•œåƒæº
mirrors=(
    "https://pypi.tuna.tsinghua.edu.cn/simple/"
    "https://mirrors.aliyun.com/pypi/simple/"
    "https://pypi.douban.com/simple/"
)

for mirror in "${mirrors[@]}"; do
    echo "å°è¯•é•œåƒæº: $mirror"
    if timeout 300 pip install -r requirements.txt -i "$mirror" --timeout 60; then
        echo "âœ… ä¾èµ–å®‰è£…æˆåŠŸ!"
        break
    else
        echo "âŒ é•œåƒæº $mirror å®‰è£…å¤±è´¥"
        if [ "$mirror" == "${mirrors[-1]}" ]; then
            echo "âŒ æ‰€æœ‰é•œåƒæºéƒ½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿žæŽ¥"
            exit 1
        fi
    fi
done

# éªŒè¯å…³é”®ä¾èµ–
echo ""
echo "ðŸ” 7. éªŒè¯å…³é”®ä¾èµ–..."
critical_packages=("torch" "numpy" "pandas" "matplotlib" "seaborn" "sklearn" "yaml")

for package in "${critical_packages[@]}"; do
    if python -c "import $package" 2>/dev/null; then
        echo "âœ… $package"
    else
        echo "âŒ $package - å¯¼å…¥å¤±è´¥"
    fi
done

# è¿è¡ŒPythonåˆå§‹åŒ–è„šæœ¬
echo ""
echo "âš™ï¸  8. è¿è¡ŒçŽ¯å¢ƒåˆå§‹åŒ–..."
if python setup_pai_dsw.py; then
    echo "âœ… PythonçŽ¯å¢ƒåˆå§‹åŒ–æˆåŠŸ"
else
    echo "âš ï¸  PythonçŽ¯å¢ƒåˆå§‹åŒ–æœ‰è­¦å‘Šï¼Œè¯·æ£€æŸ¥è¾“å‡ºä¿¡æ¯"
fi

# æ£€æŸ¥æ•°æ®æ–‡ä»¶
echo ""
echo "ðŸ’¾ 9. æ£€æŸ¥æ•°æ®æ–‡ä»¶..."
data_found=false

# æ£€æŸ¥å¸¸è§çš„æ•°æ®æ–‡ä»¶ä½ç½®
data_locations=(
    "data/raw/*.csv"
    "data/*.csv"
    "/mnt/workspace/data/*.csv"
)

for location in "${data_locations[@]}"; do
    if ls $location 1> /dev/null 2>&1; then
        echo "âœ… å‘çŽ°æ•°æ®æ–‡ä»¶: $location"
        data_found=true
        break
    fi
done

if [ "$data_found" = false ]; then
    echo "âš ï¸  æœªå‘çŽ°æ•°æ®æ–‡ä»¶ï¼Œè¯·æ‰‹åŠ¨ä¸Šä¼ è®­ç»ƒæ•°æ®åˆ°ä»¥ä¸‹ä½ç½®ä¹‹ä¸€:"
    echo "   - data/raw/"
    echo "   - data/"
    echo ""
    echo "æ”¯æŒçš„æ•°æ®æ–‡ä»¶æ ¼å¼: CSV"
fi

# åˆ›å»ºå¿«é€Ÿå¯åŠ¨è„šæœ¬
echo ""
echo "ðŸ“ 10. åˆ›å»ºå¿«é€Ÿå¯åŠ¨è„šæœ¬..."

# æ•°æ®é¢„å¤„ç†è„šæœ¬
cat > run_preprocessing.sh << 'EOF'
#!/bin/bash
echo "å¼€å§‹æ•°æ®é¢„å¤„ç†..."
cd /mnt/workspace
export PYTHONPATH=/mnt/workspace:$PYTHONPATH

python src/data/preprocess.py
echo "æ•°æ®é¢„å¤„ç†å®Œæˆ!"
EOF

# è®­ç»ƒè„šæœ¬
cat > run_training.sh << 'EOF'
#!/bin/bash
echo "å¼€å§‹æ¨¡åž‹è®­ç»ƒ..."
cd /mnt/workspace
export PYTHONPATH=/mnt/workspace:$PYTHONPATH

python src/training/train.py --config configs/lstm_transformer.yaml
echo "æ¨¡åž‹è®­ç»ƒå®Œæˆ!"
EOF

# é¢„æµ‹è„šæœ¬
cat > run_prediction.sh << 'EOF'
#!/bin/bash
echo "å¼€å§‹æ¨¡åž‹é¢„æµ‹..."
cd /mnt/workspace
export PYTHONPATH=/mnt/workspace:$PYTHONPATH

python src/inference/predict.py \
    --config configs/lstm_transformer.yaml \
    --model_path /mnt/workspace/models/best_model.pt \
    --datasets test \
    --generate_viz

echo "æ¨¡åž‹é¢„æµ‹å®Œæˆ!"
EOF

# å®Œæ•´æµæ°´çº¿è„šæœ¬
cat > run_full_pipeline.sh << 'EOF'
#!/bin/bash
echo "å¼€å§‹å®Œæ•´è®­ç»ƒæµæ°´çº¿..."
cd /mnt/workspace
export PYTHONPATH=/mnt/workspace:$PYTHONPATH

set -e

# 1. æ•°æ®é¢„å¤„ç†
echo "æ­¥éª¤1: æ•°æ®é¢„å¤„ç†"
python src/data/preprocess.py

# 2. æ¨¡åž‹è®­ç»ƒ
echo "æ­¥éª¤2: æ¨¡åž‹è®­ç»ƒ"
python src/training/train.py --config configs/lstm_transformer.yaml

# 3. æ¨¡åž‹é¢„æµ‹
echo "æ­¥éª¤3: æ¨¡åž‹é¢„æµ‹"
python src/inference/predict.py \
    --config configs/lstm_transformer.yaml \
    --model_path /mnt/workspace/models/best_model.pt \
    --datasets test val \
    --generate_viz

# 4. ç»“æžœåˆ†æž
echo "æ­¥éª¤4: ç»“æžœåˆ†æž"
python src/inference/analyze_results.py \
    --results_dir /mnt/workspace/results \
    --output_dir /mnt/workspace/analysis

# 5. ONNXå¯¼å‡º
echo "æ­¥éª¤5: ONNXå¯¼å‡º"
python src/inference/export_onnx.py \
    --model_path /mnt/workspace/models/best_model.pt \
    --output_path /mnt/workspace/models/model.onnx

echo "å®Œæ•´æµæ°´çº¿æ‰§è¡Œå®Œæˆ!"
EOF

# è®¾ç½®è„šæœ¬æƒé™
chmod +x run_*.sh

echo "âœ… å¿«é€Ÿå¯åŠ¨è„šæœ¬å·²åˆ›å»º:"
echo "   - run_preprocessing.sh  : æ•°æ®é¢„å¤„ç†"
echo "   - run_training.sh       : æ¨¡åž‹è®­ç»ƒ"  
echo "   - run_prediction.sh     : æ¨¡åž‹é¢„æµ‹"
echo "   - run_full_pipeline.sh  : å®Œæ•´æµæ°´çº¿"

# åˆ›å»ºä½¿ç”¨è¯´æ˜Ž
cat > QUICK_START.md << 'EOF'
# PAI-DSW å¿«é€Ÿå¼€å§‹æŒ‡å—

## ðŸŽ¯ é¡¹ç›®å·²æˆåŠŸéƒ¨ç½²åˆ°PAI-DSW!

### çŽ¯å¢ƒä¿¡æ¯
- å·¥ä½œç›®å½•: /mnt/workspace
- Pythonè·¯å¾„: $PYTHONPATH å·²è®¾ç½®
- é¡¹ç›®æ–‡ä»¶: å·²å®Œæ•´ä¸Šä¼ 
- ä¾èµ–åŒ…: å·²å®‰è£…

### ðŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ

#### æ–¹æ¡ˆ1: é€æ­¥æ‰§è¡Œ
```bash
# 1. æ•°æ®é¢„å¤„ç†
./run_preprocessing.sh

# 2. æ¨¡åž‹è®­ç»ƒ  
./run_training.sh

# 3. æ¨¡åž‹é¢„æµ‹
./run_prediction.sh
```

#### æ–¹æ¡ˆ2: ä¸€é”®æ‰§è¡Œå®Œæ•´æµæ°´çº¿
```bash
# å®Œæ•´è®­ç»ƒ+é¢„æµ‹+åˆ†æžæµæ°´çº¿
./run_full_pipeline.sh
```

#### æ–¹æ¡ˆ3: æ‰‹åŠ¨æ‰§è¡Œ
```bash
# è®¾ç½®çŽ¯å¢ƒ
cd /mnt/workspace
export PYTHONPATH=/mnt/workspace:$PYTHONPATH

# æ•°æ®é¢„å¤„ç†
python src/data/preprocess.py

# æ¨¡åž‹è®­ç»ƒ
python src/training/train.py --config configs/lstm_transformer.yaml

# æ¨¡åž‹é¢„æµ‹
python src/inference/predict.py \
    --config configs/lstm_transformer.yaml \
    --model_path /mnt/workspace/models/best_model.pt
```

### ðŸ“Š ç›‘æŽ§è®­ç»ƒè¿›åº¦

#### åœ¨Jupyter Notebookä¸­æŸ¥çœ‹è®­ç»ƒæ›²çº¿:
```python
import json
import matplotlib.pyplot as plt

# åŠ è½½è®­ç»ƒåŽ†å²
with open('/mnt/workspace/results/training_history.json', 'r') as f:
    history = json.load(f)

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Val Loss')
plt.legend()
plt.title('Loss Curves')

plt.subplot(1, 2, 2)
plt.plot(history['train_mae'], label='Train MAE')
plt.plot(history['val_mae'], label='Val MAE')
plt.legend() 
plt.title('MAE Curves')
plt.show()
```

### ðŸ“ é‡è¦ç›®å½•
- `/mnt/workspace/data/` - æ•°æ®æ–‡ä»¶
- `/mnt/workspace/models/` - è®­ç»ƒå¥½çš„æ¨¡åž‹
- `/mnt/workspace/results/` - è®­ç»ƒç»“æžœå’Œæ—¥å¿—
- `/mnt/workspace/analysis/` - åˆ†æžæŠ¥å‘Šå’Œå›¾è¡¨

### â“ é‡åˆ°é—®é¢˜?
1. æ£€æŸ¥ `/mnt/workspace/logs/` ç›®å½•ä¸‹çš„æ—¥å¿—æ–‡ä»¶
2. è¿è¡Œ `python setup_pai_dsw.py` é‡æ–°æ£€æŸ¥çŽ¯å¢ƒ
3. æŸ¥çœ‹ PAI_DSW_DEPLOYMENT.md èŽ·å–è¯¦ç»†éƒ¨ç½²æŒ‡å—
EOF

# æœ€ç»ˆæ€»ç»“
echo ""
echo "ðŸŽ‰ PAI-DSWéƒ¨ç½²å®Œæˆ!"
echo "============================================================"
echo "âœ… çŽ¯å¢ƒæ£€æŸ¥: é€šè¿‡"
echo "âœ… ä¾èµ–å®‰è£…: å®Œæˆ"
echo "âœ… ç›®å½•åˆ›å»º: å®Œæˆ"
echo "âœ… è„šæœ¬ç”Ÿæˆ: å®Œæˆ"

if [ "$data_found" = true ]; then
    echo "âœ… æ•°æ®æ–‡ä»¶: å·²å‘çŽ°"
    echo ""
    echo "ðŸš€ æ‚¨çŽ°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡åž‹:"
    echo "   ./run_full_pipeline.sh"
else
    echo "âš ï¸  æ•°æ®æ–‡ä»¶: è¯·æ‰‹åŠ¨ä¸Šä¼ "
    echo ""
    echo "ðŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:"
    echo "1. ä¸Šä¼ è®­ç»ƒæ•°æ®åˆ° data/raw/ ç›®å½•"
    echo "2. è¿è¡Œ: ./run_full_pipeline.sh"
fi

echo ""
echo "ðŸ“– è¯¦ç»†è¯´æ˜Žè¯·æŸ¥çœ‹: QUICK_START.md"
echo "ðŸ“š éƒ¨ç½²æ–‡æ¡£è¯·æŸ¥çœ‹: PAI_DSW_DEPLOYMENT.md"
echo ""
echo "ðŸŽ¯ é¡¹ç›®æˆåŠŸéƒ¨ç½²åœ¨: $(pwd)"
echo "============================================================"
